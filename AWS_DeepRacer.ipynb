{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWS DeepRacer.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJPn3v1UQq8MZSNxWKec5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/ReinforcementLearning/blob/master/AWS_DeepRacer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkpMmqhy3qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reward Function for staying on the track and \n",
        "#Centered\n",
        "def reward_function(params):\n",
        "    '''\n",
        "    Example of rewarding the agent to stay inside the two borders of the track\n",
        "    '''\n",
        "\n",
        "    # Read input parameters\n",
        "    all_wheels_on_track = params['all_wheels_on_track']\n",
        "    distance_from_center = params['distance_from_center']\n",
        "    track_width = params['track_width']\n",
        "\n",
        "    # Give a very low reward by default\n",
        "    reward = 1e-3\n",
        "\n",
        "    # Give a high reward if no wheels go off the track and\n",
        "    # the agent is somewhere in between the track borders\n",
        "    if all_wheels_on_track and (0.5*track_width - distance_from_center) >= 0.05:\n",
        "        reward = 1.0\n",
        "\n",
        "    # Always return a float value\n",
        "    return float(reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn5gpax_zUj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reward function for staying in the center\n",
        "'''\n",
        "This example is more specific about what kind of driving behavior to reward, \n",
        "so an agent trained with this function is likely to learn to follow the track \n",
        "very well. However, it is unlikely to learn any other behavior such as \n",
        "accelerating or braking for corners.\n",
        "'''\n",
        "def reward_function(params):\n",
        "    '''\n",
        "    Example of rewarding the agent to follow center line\n",
        "    '''\n",
        "\n",
        "    # Read input parameters\n",
        "    track_width = params['track_width']\n",
        "    distance_from_center = params['distance_from_center']\n",
        "\n",
        "    # Calculate 3 markers that are at varying distances away from the center line\n",
        "    marker_1 = 0.1 * track_width\n",
        "    marker_2 = 0.25 * track_width\n",
        "    marker_3 = 0.5 * track_width\n",
        "\n",
        "    # Give higher reward if the car is closer to center line and vice versa\n",
        "    if distance_from_center <= marker_1:\n",
        "        reward = 1.0\n",
        "    elif distance_from_center <= marker_2:\n",
        "        reward = 0.5\n",
        "    elif distance_from_center <= marker_3:\n",
        "        reward = 0.1\n",
        "    else:\n",
        "        reward = 1e-3  # likely crashed/ close to off track\n",
        "\n",
        "    return float(reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmcbCdwv0AAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The agent's only incentive is to successfully finish the track, and it has no \n",
        "incentive to drive faster or follow any particular path. It may behave erratically.\n",
        "\n",
        "However, since the reward function doesn't constrain the agent's behavior, \n",
        "it may be able to explore unexpected strategies and behaviors that turn out \n",
        "to perform well.\n",
        "'''\n",
        "\n",
        "def reward_function(params):\n",
        "    '''\n",
        "    Example of no incentive\n",
        "    '''\n",
        "\n",
        "    # Always return 1 if the car does not crash\n",
        "    return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKaqqH9m0yVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This example incentivizes the agent to follow the center line but penalizes\n",
        "with lower reward if it steers too much, which will help prevent zig-zag \n",
        "behavior. The agent will learn to drive smoothly in the simulator and likely \n",
        "display the same behavior when deployed in the physical vehicle.\n",
        "'''\n",
        "def reward_function(params):\n",
        "    '''\n",
        "    Example of penalize steering, which helps mitigate zig-zag behaviors\n",
        "    '''\n",
        "    \n",
        "    # Read input parameters\n",
        "    distance_from_center = params['distance_from_center']\n",
        "    track_width = params['track_width']\n",
        "    steering = abs(params['steering_angle']) # Only need the absolute steering angle\n",
        "\n",
        "    # Calculate 3 markers that are at varying distances away from the center line\n",
        "    marker_1 = 0.1 * track_width\n",
        "    marker_2 = 0.25 * track_width\n",
        "    marker_3 = 0.5 * track_width\n",
        "\n",
        "    # Give higher reward if the agent is closer to center line and vice versa\n",
        "    if distance_from_center <= marker_1:\n",
        "        reward = 1\n",
        "    elif distance_from_center <= marker_2:\n",
        "        reward = 0.5\n",
        "    elif distance_from_center <= marker_3:\n",
        "        reward = 0.1\n",
        "    else:\n",
        "        reward = 1e-3  # likely crashed/ close to off track\n",
        "\n",
        "    # Steering penality threshold, change the number based on your action space setting\n",
        "    ABS_STEERING_THRESHOLD = 15\n",
        "\n",
        "    # Penalize reward if the agent is steering too much\n",
        "    if steering > ABS_STEERING_THRESHOLD:\n",
        "        reward *= 0.8\n",
        "\n",
        "    return float(reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSYB6fC21CNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "We consider two factors in this reward function. First, reward the agent to\n",
        " stay inside two borders. Second, penalize the agent for getting too close to \n",
        " the next object to avoid crashes. The total reward is calculated with weighted\n",
        "  sum of the two factors. The example emphasize more on avoiding crashes but \n",
        "  you can play with different weights.\n",
        "'''\n",
        "def reward_function(params):\n",
        "    '''\n",
        "    Example of rewarding the agent to stay inside two borders\n",
        "    and penalizing getting too close to the objects in front\n",
        "    '''\n",
        "\n",
        "    all_wheels_on_track = params['all_wheels_on_track']\n",
        "    distance_from_center = params['distance_from_center']\n",
        "    track_width = params['track_width']\n",
        "    objects_distance = params['objects_distance']\n",
        "    _, next_object_index = params['closest_objects']\n",
        "    objects_left_of_center = params['objects_left_of_center']\n",
        "    is_left_of_center = params['is_left_of_center']\n",
        "\n",
        "    # Initialize reward with a small number but not zero\n",
        "    # because zero means off-track or crashed\n",
        "    reward = 1e-3\n",
        "\n",
        "    # Reward if the agent stays inside the two borders of the track\n",
        "    if all_wheels_on_track and (0.5 * track_width - distance_from_center) >= 0.05:\n",
        "        reward_lane = 1.0\n",
        "    else:\n",
        "        reward_lane = 1e-3\n",
        "\n",
        "    # Penalize if the agent is too close to the next object\n",
        "    reward_avoid = 1.0\n",
        "\n",
        "    # Distance to the next object\n",
        "    distance_closest_object = objects_distance[next_object_index]\n",
        "    # Decide if the agent and the next object is on the same lane\n",
        "    is_same_lane = objects_left_of_center[next_object_index] == is_left_of_center\n",
        "\n",
        "    if is_same_lane:\n",
        "        if 0.5 <= distance_closest_object < 0.8: \n",
        "            reward_avoid *= 0.5\n",
        "        elif 0.3 <= distance_closest_object < 0.5:\n",
        "            reward_avoid *= 0.2\n",
        "        elif distance_closest_object < 0.3:\n",
        "            reward_avoid = 1e-3 # Likely crashed\n",
        "\n",
        "    # Calculate reward by putting different weights on \n",
        "    # the two aspects above\n",
        "    reward += 1.0 * reward_lane + 4.0 * reward_avoid\n",
        "\n",
        "    return reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Uzrb7Lx_sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This reward function uses WayPoints\n",
        "#https://medium.com/proud2becloud/deepracer-our-journey-to-the-top-ten-257ff69922e\n",
        "#\n",
        "#came in 8th place\n",
        "import math\n",
        "def reward_function(params):\n",
        "    \n",
        "    track_width = params['track_width']\n",
        "    distance_from_center = params['distance_from_center']\n",
        "    steering = abs(params['steering_angle'])\n",
        "    direction_stearing=params['steering_angle']\n",
        "    speed = params['speed']\n",
        "    steps = params['steps']\n",
        "    progress = params['progress']\n",
        "    all_wheels_on_track = params['all_wheels_on_track']\n",
        "    ABS_STEERING_THRESHOLD = 15\n",
        "    SPEED_TRESHOLD = 5\n",
        "    TOTAL_NUM_STEPS = 85\n",
        "    \n",
        "    # Read input variables\n",
        "    waypoints = params['waypoints']\n",
        "    closest_waypoints = params['closest_waypoints']\n",
        "    heading = params['heading']\n",
        "    \n",
        "    reward = 1.0\n",
        "        \n",
        "    if progress == 100:\n",
        "        reward += 100\n",
        "    \n",
        "    # Calculate the direction of the center line based on the closest waypoints\n",
        "    next_point = waypoints[closest_waypoints[1]]\n",
        "    prev_point = waypoints[closest_waypoints[0]]\n",
        "    # Calculate the direction in radius, arctan2(dy, dx), the result is (-pi, pi) in radians\n",
        "    track_direction = math.atan2(next_point[1] - prev_point[1], next_point[0] - prev_point[0]) \n",
        "    # Convert to degree\n",
        "    track_direction = math.degrees(track_direction)\n",
        "    # Calculate the difference between the track direction and the heading direction of the car\n",
        "    direction_diff = abs(track_direction - heading)\n",
        "    # Penalize the reward if the difference is too large\n",
        "    DIRECTION_THRESHOLD = 10.0\n",
        "    \n",
        "    malus=1\n",
        "    \n",
        "    if direction_diff > DIRECTION_THRESHOLD:\n",
        "        malus=1-(direction_diff/50)\n",
        "        if malus<0 or malus>1:\n",
        "            malus = 0\n",
        "        reward *= malus\n",
        "    \n",
        "    return reward"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}