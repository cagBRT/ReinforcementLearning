{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model based RL with changing Environment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTWGiZ7uIQw0g9SOuf3nT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/ReinforcementLearning/blob/master/model_based_RL_with_changing_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTtKlF42hpra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDRIk2HIhs6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROWS = 6\n",
        "COLS = 9\n",
        "S = (2, 0)\n",
        "G = (0, 8)\n",
        "BLOCKS = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
        "ACTIONS = [\"left\", \"up\", \"right\", \"down\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1pCmIghchN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Maze:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = ROWS\n",
        "        self.cols = COLS\n",
        "        self.start = S\n",
        "        self.goal = G\n",
        "        self.blocks = BLOCKS\n",
        "        self.state = S\n",
        "        self.end = False\n",
        "        # init maze\n",
        "        self.maze = np.zeros((self.rows, self.cols))\n",
        "        for b in self.blocks:\n",
        "            self.maze[b] = -1\n",
        "\n",
        "    def nxtPosition(self, action):\n",
        "        r, c = self.state\n",
        "        if action == \"left\":\n",
        "            c -= 1\n",
        "        elif action == \"right\":\n",
        "            c += 1\n",
        "        elif action == \"up\":\n",
        "            r -= 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "        if (r >= 0 and r <= self.rows - 1) and (c >= 0 and c <= self.cols - 1):\n",
        "            if (r, c) not in self.blocks:\n",
        "                self.state = (r, c)\n",
        "        return self.state\n",
        "\n",
        "    def giveReward(self):\n",
        "        if self.state == self.goal:\n",
        "            self.end = True\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def showMaze(self):\n",
        "        self.maze[self.state] = 1\n",
        "        for i in range(0, self.rows):\n",
        "            print('-------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, self.cols):\n",
        "                if self.maze[i, j] == 1:\n",
        "                    token = '*'\n",
        "                if self.maze[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                if self.maze[i, j] == 0:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------------------------------')\n",
        "print(\"done\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8H-w3wSh0E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DynaAgentPlus:\n",
        "\n",
        "    def __init__(self, exp_rate=0.3, lr=0.1, n_steps=5, episodes=1, timeWeight=1e-4):\n",
        "        self.time = 0  # keep track of the total time\n",
        "        self.timeWeight = timeWeight\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.actions = ACTIONS\n",
        "        self.state_actions = []  # state & action track\n",
        "        self.exp_rate = exp_rate\n",
        "        self.lr = lr\n",
        "\n",
        "        self.steps = n_steps\n",
        "        self.episodes = episodes  # number of episodes going to play\n",
        "        self.steps_per_episode = []\n",
        "\n",
        "        self.Q_values = {}\n",
        "        # model function\n",
        "        self.model = {}\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLS):\n",
        "                self.Q_values[(row, col)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(row, col)][a] = 0\n",
        "\n",
        "    def chooseAction(self):\n",
        "        # epsilon-greedy\n",
        "        mx_nxt_reward = -999\n",
        "        action = \"\"\n",
        "\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            # greedy action\n",
        "            current_position = self.state\n",
        "            # if all actions have same value, then select randomly\n",
        "            if len(set(self.Q_values[current_position].values())) == 1:\n",
        "                action = np.random.choice(self.actions)\n",
        "            else:\n",
        "                for a in self.actions:\n",
        "                    nxt_reward = self.Q_values[current_position][a]\n",
        "                    if nxt_reward >= mx_nxt_reward:\n",
        "                        action = a\n",
        "                        mx_nxt_reward = nxt_reward\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.state_actions = []\n",
        "        self.time = 0\n",
        "\n",
        "    def updateModel(self, state, nxtState, action, reward):\n",
        "        if state not in self.model.keys():\n",
        "            self.model[state] = {}\n",
        "        for a in self.actions:\n",
        "            # the initial model for such actions was that they would\n",
        "            # lead back to the same state with a reward of 0.\n",
        "            if a != action:\n",
        "                self.model[state][a] = (0, state, 1)\n",
        "\n",
        "        self.model[state][action] = (reward, nxtState, self.time)\n",
        "\n",
        "    def play(self):\n",
        "        self.steps_per_episode = []\n",
        "\n",
        "        for ep in range(self.episodes):\n",
        "            while not self.maze.end:\n",
        "\n",
        "                action = self.chooseAction()\n",
        "                self.state_actions.append((self.state, action))\n",
        "\n",
        "                nxtState = self.maze.nxtPosition(action)\n",
        "                reward = self.maze.giveReward()\n",
        "\n",
        "                # update Q-value\n",
        "                self.Q_values[self.state][action] += self.lr * (reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[self.state][action])\n",
        "\n",
        "                # update model\n",
        "                self.updateModel(self.state, nxtState, action, reward)\n",
        "                self.state = nxtState\n",
        "                self.time += 1\n",
        "\n",
        "                # loop n times to randomly update Q-value\n",
        "                for _ in range(self.steps):\n",
        "                    # randomly choose an state\n",
        "                    rand_idx = np.random.choice(range(len(self.model.keys())))\n",
        "                    _state = list(self.model)[rand_idx]\n",
        "                    # randomly choose an action\n",
        "                    rand_idx = np.random.choice(range(len(self.model[_state].keys())))\n",
        "                    _action = list(self.model[_state])[rand_idx]\n",
        "\n",
        "                    _reward, _nxtState, _time = self.model[_state][_action]\n",
        "                    # update _reward\n",
        "                    _reward += self.timeWeight * np.sqrt(self.time - _time)\n",
        "\n",
        "                    self.Q_values[_state][_action] += self.lr * (_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action])\n",
        "            # end of game\n",
        "            if ep % 10 == 0:\n",
        "                print(\"episode\", ep)\n",
        "            self.steps_per_episode.append(len(self.state_actions))\n",
        "            self.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2QfTF_lh7wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dap = DynaAgentPlus()\n",
        "    dap.play()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eeNjPCjijPf",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb\n"
      ]
    }
  ]
}