{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model Based method.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnU/qStp2w98qrDnjz1L7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/ReinforcementLearning/blob/master/model_Based_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21_YKSeoSizy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/ReinforcementLearning.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aaqXUq3RZ5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ka3Xxe3RtIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"c2-content/c2PythonProgramming\"+str(num)+ \".png\" , width=640)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUuIEv4LSfLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"maze.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18D_hDlGRcq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up the environment\n",
        "ROWS = 6\n",
        "COLS = 9\n",
        "S = (2, 0) #start\n",
        "G = (0, 8) #goal\n",
        "BLOCKS = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
        "ACTIONS = [\"left\", \"up\", \"right\", \"down\"]\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKM7QyL8Ub2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the Maze\n",
        "\n",
        "class Maze:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.rows = ROWS\n",
        "        self.cols = COLS\n",
        "        self.start = S\n",
        "        self.goal = G\n",
        "        self.blocks = BLOCKS\n",
        "        self.state = S\n",
        "        self.end = False\n",
        "        # init maze\n",
        "        self.maze = np.zeros((self.rows, self.cols))\n",
        "        for b in self.blocks:\n",
        "            self.maze[b] = -1\n",
        "            \n",
        "    def nxtPosition(self, action):\n",
        "        r, c = self.state\n",
        "        if action == \"left\":\n",
        "            c -= 1\n",
        "        elif action == \"right\":\n",
        "            c += 1\n",
        "        elif action == \"up\":\n",
        "            r -= 1\n",
        "        else:\n",
        "            r += 1\n",
        "        \n",
        "        if (r >= 0 and r <= self.rows-1) and (c >= 0 and c <= self.cols-1):\n",
        "            if (r, c) not in self.blocks:\n",
        "                self.state = (r, c)\n",
        "        return self.state\n",
        "    \n",
        "    def giveReward(self):\n",
        "        if self.state == self.goal:\n",
        "            self.end = True\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def showMaze(self):\n",
        "        self.maze[self.state] = 1\n",
        "        for i in range(0, self.rows):\n",
        "            print('-------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, self.cols):\n",
        "                if self.maze[i, j] == 1:\n",
        "                    token = '*'\n",
        "                if self.maze[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                if self.maze[i, j] == 0:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------------------------------')\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr10hOOWRS2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the agent\n",
        "class DynaAgent:\n",
        "    \n",
        "    def __init__(self, exp_rate=0.3, lr=0.1, n_steps=5, episodes=1):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.actions = ACTIONS\n",
        "        self.state_actions = []  # state & action track\n",
        "        self.exp_rate = exp_rate\n",
        "        self.lr = lr\n",
        "\n",
        "        self.steps = n_steps\n",
        "        self.episodes = episodes  # number of episodes going to play\n",
        "        self.steps_per_episode = []\n",
        "        \n",
        "        self.Q_values = {}\n",
        "        # model function\n",
        "        self.model = {}\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLS):\n",
        "                self.Q_values[(row, col)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(row, col)][a] = 0\n",
        "        \n",
        "       \n",
        "\n",
        "        \n",
        "    def chooseAction(self):\n",
        "        # epsilon-greedy\n",
        "        mx_nxt_reward = -999\n",
        "        action = \"\"\n",
        "        \n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            # greedy action\n",
        "            current_position = self.state\n",
        "            # if all actions have same value, then select randomly\n",
        "            if len(set(self.Q_values[current_position].values())) == 1:\n",
        "                action = np.random.choice(self.actions)\n",
        "            else:\n",
        "                for a in self.actions:\n",
        "                    nxt_reward = self.Q_values[current_position][a]\n",
        "                    if nxt_reward >= mx_nxt_reward:\n",
        "                        action = a\n",
        "                        mx_nxt_reward = nxt_reward\n",
        "        return action\n",
        "    \n",
        "    def reset(self):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.state_actions = []\n",
        "    \n",
        "    def play(self):\n",
        "        self.steps_per_episode = []  \n",
        "        \n",
        "        for ep in range(self.episodes):    \n",
        "            while not self.maze.end:\n",
        "                \n",
        "                action = self.chooseAction()\n",
        "                self.state_actions.append((self.state, action))\n",
        "\n",
        "                nxtState = self.maze.nxtPosition(action)\n",
        "                reward = self.maze.giveReward()\n",
        "                # update Q-value\n",
        "                self.Q_values[self.state][action] += self.lr*(reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[self.state][action])\n",
        "\n",
        "                # update model\n",
        "                if self.state not in self.model.keys():\n",
        "                    self.model[self.state] = {}\n",
        "                self.model[self.state][action] = (reward, nxtState)\n",
        "                self.state = nxtState\n",
        "\n",
        "                # loop n times to randomly update Q-value\n",
        "                for _ in range(self.steps):\n",
        "                    # randomly choose an state\n",
        "                    rand_idx = np.random.choice(range(len(self.model.keys())))\n",
        "                    _state = list(self.model)[rand_idx]\n",
        "                    # randomly choose an action\n",
        "                    rand_idx = np.random.choice(range(len(self.model[_state].keys())))\n",
        "                    _action = list(self.model[_state])[rand_idx]\n",
        "\n",
        "                    _reward, _nxtState = self.model[_state][_action]\n",
        "\n",
        "                    self.Q_values[_state][_action] += self.lr*(_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action])       \n",
        "            # end of game\n",
        "            #self.maze.showMaze\n",
        "\n",
        "            if ep % 10 == 0:\n",
        "                print(\"episode\", ep)\n",
        "            self.steps_per_episode.append(len(self.state_actions))\n",
        "            self.reset()\n",
        "\n",
        "print(\"done\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIhgGC83UwwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N_EPISODES = 50\n",
        "    # comparison\n",
        "    agent = DynaAgent(n_steps=0, episodes=N_EPISODES)\n",
        "    agent.play()\n",
        "\n",
        "    steps_episode_0 = agent.steps_per_episode\n",
        "\n",
        "    agent = DynaAgent(n_steps=5, episodes=N_EPISODES)\n",
        "    agent.play()\n",
        "\n",
        "    steps_episode_5 = agent.steps_per_episode\n",
        "\n",
        "    agent = DynaAgent(n_steps=50, episodes=N_EPISODES)\n",
        "    agent.play()\n",
        "\n",
        "    steps_episode_50 = agent.steps_per_episode\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR2GAsr6VDjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the results\n",
        "    plt.figure(figsize=[10, 6])\n",
        "\n",
        "    plt.ylim(0, 900)\n",
        "    plt.plot(range(N_EPISODES), steps_episode_0, label=\"step=0\")\n",
        "    plt.plot(range(N_EPISODES), steps_episode_5, label=\"step=5\")\n",
        "    plt.plot(range(N_EPISODES), steps_episode_50, label=\"step=50\")\n",
        "\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XATNxcwteWwp",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8\n"
      ]
    }
  ]
}